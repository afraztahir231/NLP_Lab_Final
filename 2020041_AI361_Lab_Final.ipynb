{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583567f9",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461284ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed727bc",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d801622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/g08j8_tn54sft2dym246gf_80000gn/T/ipykernel_2091/3887519479.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('tweets.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bohmle</td>\n",
       "      <td>#GPT4 for FREE. \\nNo its not a clickbait, @Qol...</td>\n",
       "      <td>Carkingga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AI enthusiast</td>\n",
       "      <td>2019-07-03 03:44:41+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>611</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-17 18:11:12+00:00</td>\n",
       "      <td>['GPT4', 'ChatGPT4']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dan Bruno AI</td>\n",
       "      <td>ChatGPT Thinks These 5 Crypto Coins Will Explo...</td>\n",
       "      <td>Manchester, NH</td>\n",
       "      <td>The latest in #ChatGPT, #BARD, #Bing, and othe...</td>\n",
       "      <td>2021-05-19 01:19:32+00:00</td>\n",
       "      <td>470.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>5185</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-17 18:11:03+00:00</td>\n",
       "      <td>['chatgpt', 'AI', 'openAI']</td>\n",
       "      <td>dlvr.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Georgiana Comsa</td>\n",
       "      <td>New: @JWVance's post about 5 #startups (includ...</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Founder of Silicon Valley PR, award-winning PR...</td>\n",
       "      <td>2008-12-24 09:32:23+00:00</td>\n",
       "      <td>3864.0</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>2415</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-17 18:10:25+00:00</td>\n",
       "      <td>['startups', 'startup50']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitone Great</td>\n",
       "      <td>üö®Get Out!üö®\\nüí∞#Binance Spotüí∞\\n‚¨á Recommendation:...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>#ChatGPT (AI) powered Free Trading Signal! \\nL...</td>\n",
       "      <td>2022-11-21 04:42:18+00:00</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-17 18:09:39+00:00</td>\n",
       "      <td>['Binance', 'Short', 'GHSTUSDT']</td>\n",
       "      <td>rsi1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412806</th>\n",
       "      <td>(I)(AM)(T)(MOYO)</td>\n",
       "      <td>Levelsüôèüôèüôè,so happy for the chatGPT team for co...</td>\n",
       "      <td>Satoshi Island</td>\n",
       "      <td>Blockchain Enthusiast || Philanthropist || a S...</td>\n",
       "      <td>2013-12-07 16:38:30+00:00</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>4994.0</td>\n",
       "      <td>16659</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-05 17:10:31+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412807</th>\n",
       "      <td>Green</td>\n",
       "      <td>Iterating back-and-forth with tools like #Chat...</td>\n",
       "      <td>üçÅ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-03 12:49:13+00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-05 17:09:22+00:00</td>\n",
       "      <td>['ChatGPT']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412808</th>\n",
       "      <td>Gabriel Furstenheim</td>\n",
       "      <td>Russel vs ChatGPT. It's also funny that it tak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mathematician and Developer @Amazon. Previousl...</td>\n",
       "      <td>2016-07-09 21:08:52+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>169</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-05 17:09:04+00:00</td>\n",
       "      <td>['ChatGPT']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412809</th>\n",
       "      <td>Devang</td>\n",
       "      <td>Was just wondering is there any difference bet...</td>\n",
       "      <td>United States</td>\n",
       "      <td>passionate by nature, software developer by pr...</td>\n",
       "      <td>2015-05-19 03:17:06+00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>307</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-05 17:08:44+00:00</td>\n",
       "      <td>['ChatGPT', 'GPT3']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412810</th>\n",
       "      <td>Norman Meuschke</td>\n",
       "      <td>#ChatGPT and similar #LLM pose a challenge to ...</td>\n",
       "      <td>Wuppertal, Germany</td>\n",
       "      <td>Postdoc in @GippLab at @uniGoettingen \\nPh.D. ...</td>\n",
       "      <td>2011-02-22 18:03:52+00:00</td>\n",
       "      <td>149.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>451</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-05 17:08:20+00:00</td>\n",
       "      <td>['ChatGPT', 'LLM']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3412811 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_name  \\\n",
       "0                     Bohmle   \n",
       "1                        NaN   \n",
       "2               Dan Bruno AI   \n",
       "3            Georgiana Comsa   \n",
       "4               Bitone Great   \n",
       "...                      ...   \n",
       "3412806     (I)(AM)(T)(MOYO)   \n",
       "3412807                Green   \n",
       "3412808  Gabriel Furstenheim   \n",
       "3412809               Devang   \n",
       "3412810      Norman Meuschke   \n",
       "\n",
       "                                                      text  \\\n",
       "0        #GPT4 for FREE. \\nNo its not a clickbait, @Qol...   \n",
       "1                                            AI enthusiast   \n",
       "2        ChatGPT Thinks These 5 Crypto Coins Will Explo...   \n",
       "3        New: @JWVance's post about 5 #startups (includ...   \n",
       "4        üö®Get Out!üö®\\nüí∞#Binance Spotüí∞\\n‚¨á Recommendation:...   \n",
       "...                                                    ...   \n",
       "3412806  Levelsüôèüôèüôè,so happy for the chatGPT team for co...   \n",
       "3412807  Iterating back-and-forth with tools like #Chat...   \n",
       "3412808  Russel vs ChatGPT. It's also funny that it tak...   \n",
       "3412809  Was just wondering is there any difference bet...   \n",
       "3412810  #ChatGPT and similar #LLM pose a challenge to ...   \n",
       "\n",
       "                     user_location  \\\n",
       "0                        Carkingga   \n",
       "1        2019-07-03 03:44:41+00:00   \n",
       "2                   Manchester, NH   \n",
       "3                        Palo Alto   \n",
       "4                        Hong Kong   \n",
       "...                            ...   \n",
       "3412806             Satoshi Island   \n",
       "3412807                          üçÅ   \n",
       "3412808                        NaN   \n",
       "3412809              United States   \n",
       "3412810         Wuppertal, Germany   \n",
       "\n",
       "                                          user_description  \\\n",
       "0                                                      NaN   \n",
       "1                                                     60.0   \n",
       "2        The latest in #ChatGPT, #BARD, #Bing, and othe...   \n",
       "3        Founder of Silicon Valley PR, award-winning PR...   \n",
       "4        #ChatGPT (AI) powered Free Trading Signal! \\nL...   \n",
       "...                                                    ...   \n",
       "3412806  Blockchain Enthusiast || Philanthropist || a S...   \n",
       "3412807                                                NaN   \n",
       "3412808  Mathematician and Developer @Amazon. Previousl...   \n",
       "3412809  passionate by nature, software developer by pr...   \n",
       "3412810  Postdoc in @GippLab at @uniGoettingen \\nPh.D. ...   \n",
       "\n",
       "                      user_created user_followers user_friends  \\\n",
       "0                              NaN            NaN          NaN   \n",
       "1                            349.0            611        False   \n",
       "2        2021-05-19 01:19:32+00:00          470.0        157.0   \n",
       "3        2008-12-24 09:32:23+00:00         3864.0       1883.0   \n",
       "4        2022-11-21 04:42:18+00:00         1517.0        506.0   \n",
       "...                            ...            ...          ...   \n",
       "3412806  2013-12-07 16:38:30+00:00         3419.0       4994.0   \n",
       "3412807  2022-12-03 12:49:13+00:00            3.0         33.0   \n",
       "3412808  2016-07-09 21:08:52+00:00           80.0         34.0   \n",
       "3412809  2015-05-19 03:17:06+00:00           15.0         86.0   \n",
       "3412810  2011-02-22 18:03:52+00:00          149.0        262.0   \n",
       "\n",
       "                   user_favourites         user_verified  \\\n",
       "0                              NaN                   NaN   \n",
       "1        2023-05-17 18:11:12+00:00  ['GPT4', 'ChatGPT4']   \n",
       "2                             5185                 False   \n",
       "3                             2415                 False   \n",
       "4                               64                 False   \n",
       "...                            ...                   ...   \n",
       "3412806                      16659                 False   \n",
       "3412807                          3                 False   \n",
       "3412808                        169                 False   \n",
       "3412809                        307                 False   \n",
       "3412810                        451                 False   \n",
       "\n",
       "                              date                          hashtags  \\\n",
       "0                              NaN                               NaN   \n",
       "1                  Twitter Web App                               NaN   \n",
       "2        2023-05-17 18:11:03+00:00       ['chatgpt', 'AI', 'openAI']   \n",
       "3        2023-05-17 18:10:25+00:00         ['startups', 'startup50']   \n",
       "4        2023-05-17 18:09:39+00:00  ['Binance', 'Short', 'GHSTUSDT']   \n",
       "...                            ...                               ...   \n",
       "3412806  2022-12-05 17:10:31+00:00                               NaN   \n",
       "3412807  2022-12-05 17:09:22+00:00                       ['ChatGPT']   \n",
       "3412808  2022-12-05 17:09:04+00:00                       ['ChatGPT']   \n",
       "3412809  2022-12-05 17:08:44+00:00               ['ChatGPT', 'GPT3']   \n",
       "3412810  2022-12-05 17:08:20+00:00                ['ChatGPT', 'LLM']   \n",
       "\n",
       "                      source  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                    dlvr.it  \n",
       "3            Twitter Web App  \n",
       "4                       rsi1  \n",
       "...                      ...  \n",
       "3412806  Twitter for Android  \n",
       "3412807      Twitter Web App  \n",
       "3412808  Twitter for Android  \n",
       "3412809  Twitter for Android  \n",
       "3412810      Twitter Web App  \n",
       "\n",
       "[3412811 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a501af",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0d2c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows              : 3412811\n",
      "Columns           : 12\n",
      "\n",
      "Features        :\n",
      " Index(['user_name', 'text', 'user_location', 'user_description',\n",
      "       'user_created', 'user_followers', 'user_friends', 'user_favourites',\n",
      "       'user_verified', 'date', 'hashtags', 'source'],\n",
      "      dtype='object')\n",
      "\n",
      "Missing values  : 32106521\n",
      "\n",
      "Unique values   : user_name           183646\n",
      "text                474943\n",
      "user_location        41866\n",
      "user_description    187179\n",
      "user_created        185757\n",
      "user_followers       43556\n",
      "user_friends         18652\n",
      "user_favourites      51271\n",
      "user_verified           62\n",
      "date                459640\n",
      "hashtags            110543\n",
      "source                1230\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Rows              :',df.shape[0])\n",
    "print('Columns           :',df.shape[1])\n",
    "print('\\nFeatures        :\\n',df.columns)\n",
    "print('\\nMissing values  :',df.isna().sum().values.sum())\n",
    "print('\\nUnique values   :',df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc9282",
   "metadata": {},
   "source": [
    "# Remove Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd9c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rows(df):\n",
    "    duplicate_rows = df.duplicated()\n",
    "    df = df[~duplicate_rows]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589ca74",
   "metadata": {},
   "source": [
    "# Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55180d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(df):\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb4408",
   "metadata": {},
   "source": [
    "# Normalize/Clean the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ddac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].lower()              #lower case all text\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)      #get punctuations\n",
    "        text[i] = text[i].translate(translator)                #remove punctuations'\n",
    "        text[i] = emoji.demojize(text[i])                     #convert emojis to text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b09ef8",
   "metadata": {},
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9898dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    for i in range(len(text)):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = text[i].split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        text[i] = ' '.join(filtered_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1c217",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4157dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        words = word_tokenize(text[i])\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        text[i] = ' '.join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dae13",
   "metadata": {},
   "source": [
    "# Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6231593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(r'http\\S+', '', text[i])\n",
    "        text[i] = re.sub(r'www\\S+', '', text[i])\n",
    "        text[i] = re.sub(r'[^\\w\\s]', '', text[i])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442373c",
   "metadata": {},
   "source": [
    "# Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bb4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub('[^a-zA-z0-9\\s]', '', text[i])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaca2a6",
   "metadata": {},
   "source": [
    "# N Gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94226bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text):\n",
    "    ngrams_list = []\n",
    "    \n",
    "    for i in text:\n",
    "        words = text.split(' ')\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = ' '.join(words[i:i+n])\n",
    "            ngrams_list.append(ngram)\n",
    "    \n",
    "    return ngrams_list\n",
    "\n",
    "def n_gram_count(ngram):\n",
    "    counts = {}\n",
    "    \n",
    "    for i in ngram:\n",
    "        counts[i] = ngram.count(i)\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c7cf6",
   "metadata": {},
   "source": [
    "# Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d381bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(text):\n",
    "    words = []\n",
    "    \n",
    "    for i in text:\n",
    "        words.append(i.split(' '))\n",
    "        \n",
    "    plt.figure(figsize=(16,13))\n",
    "    wc=WordCloud(background_color='white',colormap='Set2',max_words=1000,max_font_size=200,width=1600,height=800)\n",
    "    wc.generate(\" \".join(words))\n",
    "    plt.title('Most discussed terms',fontsize=20)\n",
    "    plt.imshow(wc.recolor(colormap='Set2',random_state=17),alpha=0.98,interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('Word_Cloud_2020041.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533aeae",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Text Blob and RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98b03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob(text):\n",
    "    polarities = []\n",
    "\n",
    "    for i in text:\n",
    "        blob = TextBlob(text)\n",
    "\n",
    "        sentiment = blob.sentiment\n",
    "\n",
    "        polarities.append(sentiment.polarity)    \n",
    "\n",
    "    return polarities\n",
    "\n",
    "def roberta(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "    sentiments = []\n",
    "    for i in text:\n",
    "        sentiments.append(sentiment_analysis(i))\n",
    "\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ae4dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251155788a2d4d02add82f5c9adc0c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b434b3221454bfcb479db8d68aff04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b41be52c21d4e66ad895e1f3b0129ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 15:54:11.326691: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-18 15:54:11.327157: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at siebert/sentiment-roberta-large-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4733cccfab47c4b41d30a98db62a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a046e52cb0f3440ca97724c739f56eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42917e35e4f34c27b5aa22a8054287b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353eeadca7d44e1e9751cb75ab4ed6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.998561680316925}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "print(sentiment_analysis('I love you'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5decd",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7793f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of sentences\n",
    "\n",
    "def get_sentence_length(text):\n",
    "    sentence_length = {}\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        sentence_length[str(i)] = len(text[i])\n",
    "        \n",
    "    return sentence_length      #returns length of sentences w.r.t letters\n",
    "\n",
    "def word_count(text):\n",
    "    word_counts = {}\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        word_counts[str(i)] = len(text[i].split(' '))\n",
    "        \n",
    "    return word_counts\n",
    "\n",
    "def space_count(text):\n",
    "    space_counts = {}\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        space_counts[str(i)] = text[i].count(' ')\n",
    "        \n",
    "    return space_counts\n",
    "\n",
    "def verb_count(text):\n",
    "    pos_tags_count = {}\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        tokens = nltk.word_tokenize(text[i])\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "        \n",
    "        pos_tags_count[str(i)] = pos_counts\n",
    "        \n",
    "    return pos_tags_count\n",
    "\n",
    "def count_words_with_A_start(text):\n",
    "    counts = {}\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        words = text[i].split(' ')\n",
    "        \n",
    "        words_A = [word for word in words if word[0] == 'A']\n",
    "        \n",
    "        counts[str(i)] = len(words_A)\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb090",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(text):\n",
    "    count_vect = CountVectorizer()\n",
    "    \n",
    "    X = vectorizer.fit_transform(text)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
